{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP_with_PI_VI_Q.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitmankikar/MDP-with-Value-Iteration-and-Policy-Iteration/blob/master/MDP_with_PI_VI_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aqqt08WCkOSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Overview"
      ]
    },
    {
      "metadata": {
        "id": "7fZxwibkkOSo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is the area of Machine learning that deal with how an agent behave in an enviorment. This notebook covers two fundamental algorithms to solve MDPs namely **Value Iteration** and **Policy Iteration**."
      ]
    },
    {
      "metadata": {
        "id": "63UgN3urkOSp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Markov Decision Process - MDP"
      ]
    },
    {
      "metadata": {
        "id": "RnSWcWTVkOSq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In MDP, there is an agent. The agent choose an action $a_{t}$ at time $t$ and as a consequance the enviorment changes.\n",
        "Here The evniorment is world around the agent. After the action the enviorment state changes to $s_{t+1}$.\n",
        "A reward might be emitted assciated with what just happened and then this process repeats. ![](https://github.com/waqasqammar/MDP-with-Value-Iteration-and-Policy-Iteration/blob/master/nb_images/mdp.png?raw=1)\n",
        "\n",
        "So, there is a feedback cycle in that the next action you take, the next decision you make is in a situation that's the consiquence of what you did before."
      ]
    },
    {
      "metadata": {
        "id": "WZjdSFC6kOSr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "b62Efsp3kOSs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import gym.spaces as spaces\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8ycVdmNkOSv",
        "colab_type": "code",
        "outputId": "c680cb99-b4c8-46e8-ec4b-5cad8936f64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# action mapping for display the final result\n",
        "action_mapping = {\n",
        "    3: '\\u2191', # UP\n",
        "    2: '\\u2192', # RIGHT\n",
        "    1: '\\u2193', # DOWN\n",
        "    0: '\\u2190' # LEFT\n",
        "}\n",
        "print(' '.join([action_mapping[i] for i in range(4)]))\n",
        "\n",
        "\n",
        "action_mapping2 = {\n",
        "    3: 'U', # UP\n",
        "    2: 'R', # RIGHT\n",
        "    1: 'D', # DOWN\n",
        "    0: 'L' # LEFT\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "← ↓ → ↑\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kpDfdVevkOS0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Setup GYM Env for playing"
      ]
    },
    {
      "metadata": {
        "id": "Vkm2vSw8kOS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we define a faction that will take a GYM enviorment and plays number of games according to given policy."
      ]
    },
    {
      "metadata": {
        "id": "5v7J-ApokOS2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def play_episodes(enviorment, n_episodes, policy, random = False):\n",
        "    \"\"\"\n",
        "    This fucntion plays the given number of episodes given by following a policy or sample randomly from action_space.\n",
        "    \n",
        "    Parameters:\n",
        "        enviorment: openAI GYM object\n",
        "        n_episodes: number of episodes to run\n",
        "        policy: Policy to follow while playing an episode\n",
        "        random: Flag for taking random actions. if True no policy would be followed and action will be taken randomly\n",
        "        \n",
        "    Return:\n",
        "        wins: Total number of wins playing n_episodes\n",
        "        total_reward: Total reward of n_episodes\n",
        "        avg_reward: Average reward of n_episodes\n",
        "    \n",
        "    \"\"\"\n",
        "    # intialize wins and total reward\n",
        "    wins = 0\n",
        "    total_reward = 0\n",
        "    \n",
        "    # loop over number of episodes to play\n",
        "    for episode in range(n_episodes):\n",
        "        \n",
        "        # flag to check if the game is finished\n",
        "        terminated = False\n",
        "        \n",
        "        # reset the enviorment every time when playing a new episode\n",
        "        state = enviorment.reset()\n",
        "        \n",
        "        while not terminated:\n",
        "            \n",
        "            # check if the random flag is not true then follow the given policy other wise take random action\n",
        "            if random:\n",
        "                action = enviorment.action_space.sample()\n",
        "            else:\n",
        "                action = policy[state]\n",
        "\n",
        "            # take the next step\n",
        "            next_state, reward,  terminated, info = enviorment.step(action)\n",
        "            \n",
        "            #enviorment.render()\n",
        "            \n",
        "            # accumalate total reward\n",
        "            total_reward += reward\n",
        "            \n",
        "            # change the state\n",
        "            state = next_state\n",
        "            \n",
        "            # if game is over with positive reward then add 1.0 in wins\n",
        "            if terminated and reward == 1.0:\n",
        "                wins += 1\n",
        "                \n",
        "    # calculate average reward\n",
        "    average_reward = total_reward / n_episodes\n",
        "    \n",
        "    return wins, total_reward, average_reward\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H-QRpkXmkOS4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Solve for Value Iteration."
      ]
    },
    {
      "metadata": {
        "id": "qRJpm11hkOS5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/waqasqammar/MDP-with-Value-Iteration-and-Policy-Iteration/blob/master/nb_images/value_iter.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "G_WR1hEHkOS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def one_step_lookahead(env, state, V , discount_factor = 0.99):\n",
        "    \"\"\"\n",
        "    Helper function to  calculate state-value function\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object\n",
        "        state: state to consider\n",
        "        V: Estimated Value for each state. Vector of length nS\n",
        "        discount_factor: MDP discount factor\n",
        "        \n",
        "    Return:\n",
        "        action_values: Expected value of each action in a state. Vector of length nA\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize vector of action values\n",
        "    action_values = np.zeros(env.nA)\n",
        "    \n",
        "    # loop over the actions we can take in an enviorment \n",
        "    for action in range(env.nA):\n",
        "        # loop over the P_sa distribution.\n",
        "        for probablity, next_state, reward, info in env.P[state][action]:\n",
        "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
        "            action_values[action] += probablity * (reward + (discount_factor * V[next_state]))\n",
        "            \n",
        "    return action_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wyiwd6ttkOS8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_policy(env, policy, V, discount_factor):\n",
        "    \n",
        "    \"\"\"\n",
        "    Helper function to update a given policy based on given value function.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        policy: policy to update.\n",
        "        V: Estimated Value for each state. Vector of length nS.\n",
        "        discount_factor: MDP discount factor.\n",
        "    Return:\n",
        "        policy: Updated policy based on the given state-Value function 'V'.\n",
        "    \"\"\"\n",
        "    \n",
        "    for state in range(env.nS):\n",
        "        # for a given state compute state-action value.\n",
        "        action_values = one_step_lookahead(env, state, V, discount_factor)\n",
        "        \n",
        "        # choose the action which maximizez the state-action value.\n",
        "        policy[state] =  np.argmax(action_values)\n",
        "        \n",
        "    return policy\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1weDgCykOS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def value_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
        "    \"\"\"\n",
        "    Algorithm to solve MPD.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        discount_factor: MDP discount factor.\n",
        "        max_iteration: Maximum No.  of iterations to run.\n",
        "        \n",
        "    Return:\n",
        "        V: Optimal state-Value function. Vector of lenth nS.\n",
        "        optimal_policy: Optimal policy. Vector of length nS.\n",
        "    \n",
        "    \"\"\"\n",
        "    # intialize value fucntion\n",
        "    V = np.zeros(env.nS)\n",
        "    \n",
        "    # iterate over max_iterations\n",
        "    for i in range(max_iteration):\n",
        "        \n",
        "        #  keep track of change with previous value function\n",
        "        prev_v = np.copy(V) \n",
        "    \n",
        "        # loop over all states\n",
        "        for state in range(env.nS):\n",
        "            \n",
        "            # Asynchronously update the state-action value\n",
        "            #action_values = one_step_lookahead(env, state, V, discount_factor)\n",
        "            \n",
        "            # Synchronously update the state-action value\n",
        "            action_values = one_step_lookahead(env, state, prev_v, discount_factor)\n",
        "            \n",
        "            # select best action to perform based on highest state-action value\n",
        "            best_action_value = np.max(action_values)\n",
        "            \n",
        "            # update the current state-value fucntion\n",
        "            V[state] =  best_action_value\n",
        "            \n",
        "        # if policy not changed over 10 iterations it converged.\n",
        "        if i % 10 == 0:\n",
        "            # if values of 'V' not changing after one iteration\n",
        "            if (np.all(np.isclose(V, prev_v))):\n",
        "                print('Value converged at iteration %d' %(i+1))\n",
        "                break\n",
        "\n",
        "    # intialize optimal policy\n",
        "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
        "    \n",
        "    # update the optimal polciy according to optimal value function 'V'\n",
        "    optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
        "    \n",
        "    return V, optimal_policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vGCiuHt3kOTB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test the Algorithim"
      ]
    },
    {
      "metadata": {
        "id": "wTC6fXtrkOTC",
        "colab_type": "code",
        "outputId": "fab59cc1-5d2a-4b75-ef15-b5af8aa19a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "enviorment = gym.make('FrozenLake8x8-v0')\n",
        "tic = time.time()\n",
        "opt_V, opt_Policy = value_iteration(enviorment.env, max_iteration = 1000)\n",
        "toc = time.time()\n",
        "elapsed_time = (toc - tic) * 1000\n",
        "print (f\"Time to converge: {elapsed_time: 0.3} ms\")\n",
        "print('Optimal Value function: ')\n",
        "print(opt_V.reshape((8, 8)))\n",
        "print('Final Policy: ')\n",
        "tmp = opt_Policy.reshape((8, 8))\n",
        "#print(tmp)\n",
        "\n",
        "for r in range(len(tmp)):\n",
        "  print(' '.join([action_mapping2[int(action)] for action in tmp[r]]))  \n",
        "\n",
        "#print('\\t\\t\\t'.join([action_mapping[int(action)] for action in opt_Policy]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value converged at iteration 601\n",
            "Time to converge:  5.07e+02 ms\n",
            "Optimal Value function: \n",
            "[[0.89251438 0.89520026 0.89919342 0.90376571 0.90866503 0.91368771\n",
            "  0.9185209  0.92231843]\n",
            " [0.8918656  0.8939008  0.89732647 0.90158507 0.90637534 0.91160223\n",
            "  0.91748525 0.92509147]\n",
            " [0.87639611 0.86213473 0.81870388 0.         0.77762511 0.86519756\n",
            "  0.90896731 0.93064573]\n",
            " [0.86356823 0.8113023  0.6991162  0.420497   0.56364163 0.\n",
            "  0.8815037  0.93899761]\n",
            " [0.85334708 0.71065134 0.46945033 0.         0.49449902 0.56831481\n",
            "  0.799196   0.95017178]\n",
            " [0.84570508 0.         0.         0.1526892  0.35302817 0.41295826\n",
            "  0.         0.96420128]\n",
            " [0.84062173 0.         0.164127   0.10549919 0.         0.31877351\n",
            "  0.         0.98112763]\n",
            " [0.83808344 0.61180435 0.38737881 0.         0.27175197 0.54432026\n",
            "  0.7715021  0.        ]]\n",
            "Final Policy: \n",
            "U R R R R R R R\n",
            "U U U U U U U R\n",
            "L U L L R U R R\n",
            "L L L D L L R R\n",
            "L U L L R D U R\n",
            "L L L D U L L R\n",
            "L L D L L L L R\n",
            "L D L L D R D L\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VtbB4XtfkOTF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_episode = 1000\n",
        "wins, total_reward, avg_reward = play_episodes(enviorment, n_episode, opt_Policy, random = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZKNNqa8CkOTI",
        "colab_type": "code",
        "outputId": "8222ade0-d9e7-40ce-a8aa-a6c082eedc53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(f'Total wins with value iteration: {wins}')\n",
        "print(f\"Average rewards with value iteration: {avg_reward}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total wins with value iteration: 847\n",
            "Average rewards with value iteration: 0.847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6iShamU9kOTK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Solve for Policy Iteration"
      ]
    },
    {
      "metadata": {
        "id": "f9xpY9YokOTL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/waqasqammar/MDP-with-Value-Iteration-and-Policy-Iteration/blob/master/nb_images/policy_iter.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "kr74xDmpkOTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def policy_eval(env, policy, V, discount_factor):\n",
        "    \"\"\"\n",
        "    Helper function to evaluate a policy.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        policy: policy to evaluate.\n",
        "        V: Estimated Value for each state. Vector of length nS.\n",
        "        discount_factor: MDP discount factor.\n",
        "    Return:\n",
        "        policy_value: Estimated value of each state following a given policy and state-value 'V'. \n",
        "        \n",
        "    \"\"\"\n",
        "    policy_value = np.zeros(env.nS)\n",
        "    for state, action in enumerate(policy):\n",
        "        for probablity, next_state, reward, info in env.P[state][action]:\n",
        "            policy_value[state] += probablity * (reward + (discount_factor * V[next_state]))\n",
        "            \n",
        "    return policy_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDV1UyZlkOTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, discount_factor = 0.9999, max_iteration = 1000):\n",
        "    \"\"\"\n",
        "    Algorithm to solve MPD.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        discount_factor: MDP discount factor.\n",
        "        max_iteration: Maximum No.  of iterations to run.\n",
        "        \n",
        "    Return:\n",
        "        V: Optimal state-Value function. Vector of lenth nS.\n",
        "        new_policy: Optimal policy. Vector of length nS.\n",
        "    \n",
        "    \"\"\"\n",
        "    # intialize the state-Value function\n",
        "    V = np.zeros(env.nS)\n",
        "    \n",
        "    # intialize a random policy\n",
        "    policy = np.random.randint(0, 4, env.nS)\n",
        "    policy_prev = np.copy(policy)\n",
        "    \n",
        "    for i in range(max_iteration):\n",
        "        \n",
        "        # evaluate given policy\n",
        "        V = policy_eval(env, policy, V, discount_factor)\n",
        "        \n",
        "        # improve policy\n",
        "        policy = update_policy(env, policy, V, discount_factor)\n",
        "        \n",
        "        # if policy not changed over 10 iterations it converged.\n",
        "        if i % 10 == 0:\n",
        "            if (np.all(np.equal(policy, policy_prev))):\n",
        "                print('policy converged at iteration %d' %(i+1))\n",
        "                break\n",
        "            policy_prev = np.copy(policy)\n",
        "            \n",
        "\n",
        "            \n",
        "    return V, policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6JzUjn86kOTQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Policy Iteration"
      ]
    },
    {
      "metadata": {
        "id": "g_0gDETUkOTQ",
        "colab_type": "code",
        "outputId": "0543260a-d79e-4599-83f4-7ca1505e55b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "enviorment2 = gym.make('FrozenLake8x8-v0')\n",
        "tic = time.time()\n",
        "opt_V2, opt_policy2 = policy_iteration(enviorment2.env, discount_factor = 0.999, max_iteration = 10000)\n",
        "toc = time.time()\n",
        "elapsed_time = (toc - tic) * 1000\n",
        "print (f\"Time to converge: {elapsed_time: 0.3} ms\")\n",
        "print('Optimal Value function: ')\n",
        "print(opt_V2.reshape((8, 8)))\n",
        "print('Final Policy: ')\n",
        "#print(opt_policy2)\n",
        "#print(' '.join([action_mapping[(action)] for action in opt_policy2]))\n",
        "\n",
        "tmp = opt_policy2.reshape((8, 8))\n",
        "#print(tmp)\n",
        "\n",
        "for r in range(len(tmp)):\n",
        "  print(' '.join([action_mapping2[int(action)] for action in tmp[r]]))  \n",
        "\n",
        "#print('\\t\\t\\t'.join([action_mapping[int(action)] for action in opt_Policy]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "policy converged at iteration 71\n",
            "Time to converge:  59.0 ms\n",
            "Optimal Value function: \n",
            "[[0.41662507 0.44211228 0.4790079  0.51930038 0.55941994 0.59533683\n",
            "  0.6186486  0.62520455]\n",
            " [0.41036321 0.42979845 0.46192668 0.50079507 0.54343069 0.59046392\n",
            "  0.6300175  0.64290514]\n",
            " [0.38711423 0.38955309 0.37656062 0.         0.47308683 0.56398111\n",
            "  0.64566115 0.67252113]\n",
            " [0.35084055 0.33942226 0.30107344 0.2064086  0.32890866 0.\n",
            "  0.6503424  0.71476906]\n",
            " [0.29777629 0.26746406 0.1851217  0.         0.31994334 0.40543279\n",
            "  0.60489118 0.77107742]\n",
            " [0.23137163 0.         0.         0.09117882 0.23524157 0.3011139\n",
            "  0.         0.83938889]\n",
            " [0.18751728 0.         0.03479682 0.04123882 0.         0.26924241\n",
            "  0.         0.91705258]\n",
            " [0.16575624 0.10868456 0.06701109 0.         0.25414113 0.50993283\n",
            "  0.75389126 0.        ]]\n",
            "Final Policy: \n",
            "U R R R R R R R\n",
            "U U U U U R R D\n",
            "U U L L R U R D\n",
            "U U U D L L R R\n",
            "U U L L R D U R\n",
            "L L L D U L L R\n",
            "L L D L L L L R\n",
            "L D L L D R D L\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zVU-c0IzkOTT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_episode = 1000\n",
        "wins, total_reward, avg_reward = play_episodes(enviorment2, n_episode, opt_policy2, random = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66r_QGcTkOTX",
        "colab_type": "code",
        "outputId": "d886949b-5f32-48d6-c799-c5a475c6982e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(f'Total wins with Policy iteration: {wins}')\n",
        "print(f\"Average rewards with Policy iteration: {avg_reward}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total wins with Policy iteration: 853\n",
            "Average rewards with Policy iteration: 0.853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pe1mlVVSkOTa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remarks"
      ]
    },
    {
      "metadata": {
        "id": "RoyRjE-WkOTb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Policy Iteration converge faster but takes more computation."
      ]
    },
    {
      "metadata": {
        "id": "aVdbwdzEkOTf",
        "colab_type": "code",
        "outputId": "26203141-1ca4-4e51-e614-e64de1882798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/Lodur03/Q-Learning-Taxi-v2/blob/master/Q-Learning-Taxi.ipynb\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"FrozenLake8x8-v0\") # Create environment\n",
        "env.render() # Show it"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f_I1RwCibm-7",
        "colab_type": "code",
        "outputId": "4486e2d5-10c1-461d-dd48-47ec35387c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Number of possible actions\n",
        "action_size = env.action_space.n \n",
        "print(\"Action size \", action_size) \n",
        "\n",
        "# Number of possible states\n",
        "state_size = env.observation_space.n \n",
        "print(\"State size \", state_size)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action size  4\n",
            "State size  64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XoE4Ry1fbpw5",
        "colab_type": "code",
        "outputId": "c334ac06-2f50-4e78-b9aa-62b6dbfccac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7aiafcyXbyAy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "episodes = 30000            # Total episodes\n",
        "max_steps = 1000            # Max steps per episode\n",
        "lr = 0.3                    # Learning rate\n",
        "decay_fac = 0.00001         # Decay learning rate each iteration\n",
        "gamma = 0.90                # Discounting rate - later rewards impact less"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPLVLu7Cb1aQ",
        "colab_type": "code",
        "outputId": "615e602d-bea3-435e-87c4-f7eeebfbd7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "for episode in range(episodes):\n",
        "    \n",
        "    state = env.reset() # Reset the environment\n",
        "    done = False        # Are we done with the environment\n",
        "    lr -= decay_fac     # Decaying learning rate\n",
        "    step = 0\n",
        "    \n",
        "    if lr <= 0: # Nothing more to learn?\n",
        "        break\n",
        "        \n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Randomly Choose an Action\n",
        "        action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action -> observe new state and reward\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        # Update qtable values\n",
        "        if done == True: # If last, do not count future accumulated reward\n",
        "            if(step < 199 | step > 201):\n",
        "                qtable[state, action] = qtable[state, action]+lr*(reward+gamma*0-qtable[state,action])\n",
        "            break\n",
        "        else: # Consider accumulated reward of best decision stream\n",
        "            qtable[state, action] = qtable[state,action]+lr*(reward+gamma*np.max(qtable[new_state,:])-qtable[state,action])\n",
        "    \n",
        "        # if done.. jump to next episode\n",
        "        if done == True:\n",
        "            break\n",
        "        \n",
        "        # moving states\n",
        "        state = new_state\n",
        "        \n",
        "    episode += 1\n",
        "    \n",
        "    if (episode % 3000 == 0):\n",
        "        print('episode = ', episode)\n",
        "        print('learning rate = ', lr)\n",
        "        print('-----------')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode =  3000\n",
            "learning rate =  0.26999999999997\n",
            "-----------\n",
            "episode =  6000\n",
            "learning rate =  0.23999999999993998\n",
            "-----------\n",
            "episode =  9000\n",
            "learning rate =  0.20999999999990998\n",
            "-----------\n",
            "episode =  12000\n",
            "learning rate =  0.17999999999987998\n",
            "-----------\n",
            "episode =  15000\n",
            "learning rate =  0.14999999999984998\n",
            "-----------\n",
            "episode =  18000\n",
            "learning rate =  0.11999999999982693\n",
            "-----------\n",
            "episode =  21000\n",
            "learning rate =  0.08999999999983856\n",
            "-----------\n",
            "episode =  24000\n",
            "learning rate =  0.059999999999848445\n",
            "-----------\n",
            "episode =  27000\n",
            "learning rate =  0.029999999999839697\n",
            "-----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GwWPCSFpb6bm",
        "colab_type": "code",
        "outputId": "1a24cfe5-146f-4a04-b774-4218adb6c617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# New environment\n",
        "state = env.reset()\n",
        "env.render()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while(done == False):\n",
        "    \n",
        "    action = np.argmax(qtable[state,:]) # Choose best action (Q-table)\n",
        "    state, reward, done, info = env.step(action) # Take action\n",
        "    total_reward += reward  # Summing rewards\n",
        "    \n",
        "    # Display it\n",
        "    time.sleep(0.5)\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    print('Episode Reward = ', total_reward)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "Episode Reward =  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1W8EBVBHct2_",
        "colab_type": "code",
        "outputId": "bacefe9d-5530-4048-88ee-753b232ff986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "#print(qtable.shape)\n",
        "\n",
        "tmp1 = np.zeros(qtable.shape[0])\n",
        "\n",
        "for state in range(qtable.shape[0]):\n",
        "  tmp1[state] = int(np.argmax( qtable[state,:] ) )\n",
        "                \n",
        "tmp1 = tmp1.reshape(8, 8)\n",
        "\n",
        "for r in range(len(tmp1[0])):\n",
        "  print(' '.join([action_mapping2[int(action)] for action in tmp1[r]]))  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U R R R R R R R\n",
            "U R U U R R R D\n",
            "U U L L R U R D\n",
            "U U U D L L R D\n",
            "U U L L R D R R\n",
            "L L L D U L L R\n",
            "L L D R L R L R\n",
            "L D L L R R U L\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}